{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Imports"
      ],
      "metadata": {
        "id": "ja9hzdkf-oMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install alpha_vantage\n",
        "!pip install transformers==4.29.2\n",
        "!pip install rwkv\n",
        "!pip install yfinance\n",
        "!pip install optuna"
      ],
      "metadata": {
        "id": "B6robljlMnuN",
        "execution": {
          "iopub.status.busy": "2023-06-24T11:22:29.209529Z",
          "iopub.execute_input": "2023-06-24T11:22:29.210633Z",
          "iopub.status.idle": "2023-06-24T11:23:34.664087Z",
          "shell.execute_reply.started": "2023-06-24T11:22:29.210593Z",
          "shell.execute_reply": "2023-06-24T11:23:34.662792Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###======= Imports =======###\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn import preprocessing\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "from alpha_vantage.timeseries import TimeSeries\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "# from CustomRWKV import RwkvConfig, RwkvModel, RwkvPreTrainedModel\n",
        "from rwkv.model import RWKV\n",
        "import math\n",
        "import yfinance as yf\n",
        "import optuna\n"
      ],
      "metadata": {
        "id": "8xVgCEy1Mf3O",
        "execution": {
          "iopub.status.busy": "2023-06-24T11:24:00.895753Z",
          "iopub.execute_input": "2023-06-24T11:24:00.896127Z",
          "iopub.status.idle": "2023-06-24T11:24:06.891961Z",
          "shell.execute_reply.started": "2023-06-24T11:24:00.896098Z",
          "shell.execute_reply": "2023-06-24T11:24:06.890874Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Rwkv"
      ],
      "metadata": {
        "id": "2282Bsw_-xhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2023 Bo Peng and HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"PyTorch RWKV model.\"\"\"\n",
        "\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import List, Optional, Tuple, Union\n",
        "\n",
        "import torch\n",
        "import torch.utils.checkpoint\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "from transformers.modeling_utils import PreTrainedModel\n",
        "from transformers.utils import (\n",
        "    ModelOutput,\n",
        "    add_code_sample_docstrings,\n",
        "    add_start_docstrings,\n",
        "    add_start_docstrings_to_model_forward,\n",
        "    is_ninja_available,\n",
        "    is_torch_cuda_available,\n",
        "    logging,\n",
        ")\n",
        "from transformers.models.rwkv.configuration_rwkv import RwkvConfig\n",
        "\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "_CHECKPOINT_FOR_DOC = \"RWKV/rwkv-4-169m-pile\"\n",
        "_CONFIG_FOR_DOC = \"RwkvConfig\"\n",
        "\n",
        "RWKV_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
        "    \"RWKV/rwkv-4-169m-pile\",\n",
        "    \"RWKV/rwkv-4-430m-pile\",\n",
        "    \"RWKV/rwkv-4-1b5-pile\",\n",
        "    \"RWKV/rwkv-4-3b-pile\",\n",
        "    \"RWKV/rwkv-4-7b-pile\",\n",
        "    \"RWKV/rwkv-4-14b-pile\",\n",
        "    \"RWKV/rwkv-raven-1b5\",\n",
        "    \"RWKV/rwkv-raven-3b\",\n",
        "    \"RWKV/rwkv-raven-7b\",\n",
        "    \"RWKV/rwkv-raven-14b\",\n",
        "    # See all RWKV models at https://huggingface.co/models?filter=rwkv\n",
        "]\n",
        "\n",
        "\n",
        "rwkv_cuda_kernel = None\n",
        "\n",
        "\n",
        "def load_wkv_cuda_kernel(context_length):\n",
        "    from torch.utils.cpp_extension import load as load_kernel\n",
        "\n",
        "    global rwkv_cuda_kernel\n",
        "\n",
        "    kernel_folder = Path(__file__).resolve().parent.parent.parent / \"kernels\" / \"rwkv\"\n",
        "    cuda_kernel_files = [kernel_folder / f for f in [\"wkv_op.cpp\", \"wkv_cuda.cu\", \"wkv_cuda_bf16.cu\"]]\n",
        "\n",
        "    # Only load the kernel if it's not been loaded yet or if we changed the context length\n",
        "    if rwkv_cuda_kernel is not None and rwkv_cuda_kernel.max_seq_length == context_length:\n",
        "        return\n",
        "\n",
        "    logger.info(f\"Loading CUDA kernel for RWKV at context length of {context_length}.\")\n",
        "\n",
        "    flags = [\n",
        "        \"-res-usage\",\n",
        "        \"--maxrregcount 60\",\n",
        "        \"--use_fast_math\",\n",
        "        \"-O3\",\n",
        "        \"-Xptxas -O3\",\n",
        "        \"--extra-device-vectorization\",\n",
        "        f\"-DTmax={context_length}\",\n",
        "    ]\n",
        "    rwkv_cuda_kernel = load_kernel(\n",
        "        name=f\"wkv_{context_length}\",\n",
        "        sources=cuda_kernel_files,\n",
        "        verbose=(logging.get_verbosity() == logging.DEBUG),\n",
        "        extra_cuda_cflags=flags,\n",
        "    )\n",
        "    rwkv_cuda_kernel.max_seq_length = context_length\n",
        "\n",
        "\n",
        "class RwkvLinearAttention(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, time_decay, time_first, key, value, state=None, return_state=False):\n",
        "        batch_size, seq_len, hidden_size = key.size()\n",
        "        if seq_len > rwkv_cuda_kernel.max_seq_length:\n",
        "            raise ValueError(\n",
        "                f\"Cannot process a batch with {seq_len} tokens at the same time, use a maximum of \"\n",
        "                f\"{rwkv_cuda_kernel.max_seq_length} with this model.\"\n",
        "            )\n",
        "        if batch_size * hidden_size % min(hidden_size, 32) != 0:\n",
        "            raise ValueError(\n",
        "                f\"The product of batch size ({batch_size}) and hidden size ({hidden_size}) needs to be a round \"\n",
        "                f\"multiple of {min(hidden_size, 32)}.\"\n",
        "            )\n",
        "\n",
        "        ctx.input_dtype = key.dtype\n",
        "\n",
        "        if (\n",
        "            time_decay.device.type != \"cuda\"\n",
        "            or time_first.device.type != \"cuda\"\n",
        "            or key.device.type != \"cuda\"\n",
        "            or value.device.type != \"cuda\"\n",
        "        ):\n",
        "            raise ValueError(\"Calling the CUDA kernel for wkv attention requires all tensors to be on CUDA devices.\")\n",
        "\n",
        "        time_decay = -torch.exp(time_decay.float().contiguous())\n",
        "        if key.dtype == torch.float16:\n",
        "            time_first = time_first.float()\n",
        "            key = key.float()\n",
        "            value = value.float()\n",
        "        time_first = time_first.contiguous()\n",
        "        key = key.contiguous()\n",
        "        value = value.contiguous()\n",
        "        # The CUDA kernel will fill this tensor.\n",
        "        output = torch.empty_like(key, memory_format=torch.contiguous_format)\n",
        "        if return_state or state is not None:\n",
        "            if state is None:\n",
        "                state = torch.zeros(\n",
        "                    batch_size,\n",
        "                    hidden_size,\n",
        "                    3,\n",
        "                    dtype=torch.float32,\n",
        "                    device=key.device,\n",
        "                    memory_format=torch.contiguous_format,\n",
        "                )\n",
        "                state[:, :, 2] -= 1e38\n",
        "            else:\n",
        "                state = torch.cat([s.unsqueeze(2) for s in state], dim=2).contiguous()\n",
        "            if key.dtype == torch.bfloat16:\n",
        "                forward_func = rwkv_cuda_kernel.forward_with_state_bf16\n",
        "            else:\n",
        "                forward_func = rwkv_cuda_kernel.forward_with_state\n",
        "            forward_func(time_decay, time_first, key, value, output, state)\n",
        "        else:\n",
        "            forward_func = rwkv_cuda_kernel.forward_bf16 if key.dtype == torch.bfloat16 else rwkv_cuda_kernel.forward\n",
        "            forward_func(time_decay, time_first, key, value, output)\n",
        "\n",
        "        ctx.save_for_backward(time_decay, time_first, key, value, output)\n",
        "\n",
        "        if state is not None:\n",
        "            state = [s.squeeze(2) for s in torch.chunk(state, 3, dim=2)]\n",
        "\n",
        "        return output.to(ctx.input_dtype), state\n",
        "\n",
        "    @staticmethod\n",
        "    # g stands for grad\n",
        "    def backward(ctx, g_output):\n",
        "        input_dtype = ctx.input_dtype\n",
        "\n",
        "        time_decay, time_first, key, value, output = ctx.saved_tensors\n",
        "        # The CUDA kernel will fill those tensors.\n",
        "        g_time_decay = torch.empty_like(\n",
        "            time_decay,\n",
        "            memory_format=torch.contiguous_format,\n",
        "            dtype=torch.bfloat16 if input_dtype == torch.bfloat16 else torch.float32,\n",
        "        )\n",
        "        g_time_first = torch.empty_like(time_first, memory_format=torch.contiguous_format)\n",
        "        g_key = torch.empty_like(key, memory_format=torch.contiguous_format)\n",
        "        g_value = torch.empty_like(value, memory_format=torch.contiguous_format)\n",
        "\n",
        "        if input_dtype == torch.float16:\n",
        "            g_output = g_output.float()\n",
        "        backward_func = rwkv_cuda_kernel.backward_bf16 if input_dtype == torch.bfloat16 else rwkv_cuda_kernel.backward\n",
        "        backward_func(\n",
        "            time_decay,\n",
        "            time_first,\n",
        "            key,\n",
        "            value,\n",
        "            output,\n",
        "            g_output.contiguous(),\n",
        "            g_time_decay,\n",
        "            g_time_first,\n",
        "            g_key,\n",
        "            g_value,\n",
        "        )\n",
        "        g_time_decay = torch.sum(g_time_decay, dim=0)\n",
        "        g_time_first = torch.sum(g_time_first, dim=0)\n",
        "\n",
        "        return (\n",
        "            None,\n",
        "            None,\n",
        "            None,\n",
        "            g_time_decay.to(input_dtype),\n",
        "            g_time_first.to(input_dtype),\n",
        "            g_key.to(input_dtype),\n",
        "            g_value.to(input_dtype),\n",
        "        )\n",
        "\n",
        "\n",
        "def rwkv_linear_attention_cpu(time_decay, time_first, key, value, state=None, return_state=False):\n",
        "    # For CPU fallback. Will be slower and probably take more memory than the custom CUDA kernel if not executed\n",
        "    # within a torch.no_grad.\n",
        "    _, seq_length, _ = key.size()\n",
        "    output = torch.zeros_like(key)\n",
        "\n",
        "    if state is None:\n",
        "        num_state = torch.zeros_like(key[:, 0], dtype=torch.float32)\n",
        "        den_state = torch.zeros_like(key[:, 0], dtype=torch.float32)\n",
        "        max_state = torch.zeros_like(key[:, 0], dtype=torch.float32) - 1e38\n",
        "    else:\n",
        "        num_state, den_state, max_state = state\n",
        "    # For numerical stability\n",
        "    #    real_numerator_state = num_state * torch.exp(max_state)\n",
        "    #    real_denominator_state = den_state * torch.exp(max_state)\n",
        "\n",
        "    time_decay = -torch.exp(time_decay)\n",
        "\n",
        "    for current_index in range(seq_length):\n",
        "        current_key = key[:, current_index].float()\n",
        "        current_value = value[:, current_index]\n",
        "\n",
        "        # wkv computation at time t\n",
        "        max_for_output = torch.maximum(max_state, current_key + time_first)\n",
        "        e1 = torch.exp(max_state - max_for_output)\n",
        "        e2 = torch.exp(current_key + time_first - max_for_output)\n",
        "        numerator = e1 * num_state + e2 * current_value\n",
        "        denominator = e1 * den_state + e2\n",
        "        output[:, current_index] = (numerator / denominator).to(output.dtype)\n",
        "\n",
        "        # Update state for next iteration\n",
        "        max_for_state = torch.maximum(max_state + time_decay, current_key)\n",
        "        e1 = torch.exp(max_state + time_decay - max_for_state)\n",
        "        e2 = torch.exp(current_key - max_for_state)\n",
        "        num_state = e1 * num_state + e2 * current_value\n",
        "        den_state = e1 * den_state + e2\n",
        "        max_state = max_for_state\n",
        "\n",
        "    if return_state or state is not None:\n",
        "        state = [num_state, den_state, max_state]\n",
        "\n",
        "    return output, state\n",
        "\n",
        "\n",
        "def rwkv_linear_attention(time_decay, time_first, key, value, state=None, return_state=False):\n",
        "    no_cuda = any(t.device.type != \"cuda\" for t in [time_decay, time_first, key, value])\n",
        "    # Launching the CUDA kernel for just one token will actually be slower (there is no for loop in the CPU version\n",
        "    # in this case).\n",
        "    one_token = key.size(1) == 1\n",
        "    if rwkv_cuda_kernel is None or no_cuda or one_token:\n",
        "        return rwkv_linear_attention_cpu(time_decay, time_first, key, value, state=state, return_state=return_state)\n",
        "    else:\n",
        "        return RwkvLinearAttention.apply(time_decay, time_first, key, value, state, return_state)\n",
        "\n",
        "\n",
        "class RwkvSelfAttention(nn.Module):\n",
        "    def __init__(self, config, layer_id=0):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        kernel_loaded = rwkv_cuda_kernel is not None and rwkv_cuda_kernel.max_seq_length == config.context_length\n",
        "        if is_ninja_available() and is_torch_cuda_available() and not kernel_loaded:\n",
        "            try:\n",
        "                load_wkv_cuda_kernel(config.context_length)\n",
        "            except Exception:\n",
        "                logger.info(\"Could not load the custom CUDA kernel for RWKV attention.\")\n",
        "        self.layer_id = layer_id\n",
        "        hidden_size = config.hidden_size\n",
        "        attention_hidden_size = (\n",
        "            config.attention_hidden_size if config.attention_hidden_size is not None else hidden_size\n",
        "        )\n",
        "        self.attention_hidden_size = attention_hidden_size\n",
        "\n",
        "        self.time_decay = nn.Parameter(torch.empty(attention_hidden_size))\n",
        "        self.time_first = nn.Parameter(torch.empty(attention_hidden_size))\n",
        "\n",
        "        self.time_mix_key = nn.Parameter(torch.empty(1, 1, hidden_size))\n",
        "        self.time_mix_value = nn.Parameter(torch.empty(1, 1, hidden_size))\n",
        "        self.time_mix_receptance = nn.Parameter(torch.empty(1, 1, hidden_size))\n",
        "\n",
        "        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n",
        "        self.key = nn.Linear(hidden_size, attention_hidden_size, bias=False)\n",
        "        self.value = nn.Linear(hidden_size, attention_hidden_size, bias=False)\n",
        "        self.receptance = nn.Linear(hidden_size, attention_hidden_size, bias=False)\n",
        "        self.output = nn.Linear(attention_hidden_size, hidden_size, bias=False)\n",
        "\n",
        "    # TODO: maybe jit, otherwise move inside forward\n",
        "    def extract_key_value(self, hidden, state=None):\n",
        "        # Mix hidden with the previous timestep to produce key, value, receptance\n",
        "        if hidden.size(1) == 1 and state is not None:\n",
        "            shifted = state[1][:, :, self.layer_id]\n",
        "        else:\n",
        "            shifted = self.time_shift(hidden)\n",
        "            if state is not None:\n",
        "                shifted[:, 0] = state[1][:, :, self.layer_id].squeeze()\n",
        "        key = hidden * self.time_mix_key + shifted * (1 - self.time_mix_key)\n",
        "        value = hidden * self.time_mix_value + shifted * (1 - self.time_mix_value)\n",
        "        receptance = hidden * self.time_mix_receptance + shifted * (1 - self.time_mix_receptance)\n",
        "\n",
        "        key = self.key(key)\n",
        "        value = self.value(value)\n",
        "        receptance = torch.sigmoid(self.receptance(receptance))\n",
        "        if state is not None:\n",
        "            state[1][:, :, self.layer_id] = hidden[:, -1]\n",
        "        return receptance, key, value, state\n",
        "\n",
        "    def forward(self, hidden, state=None, use_cache=False):\n",
        "        receptance, key, value, state = self.extract_key_value(hidden, state=state)\n",
        "        layer_state = tuple(s[:, :, self.layer_id] for s in state[2:]) if state is not None else None\n",
        "        rwkv, layer_state = rwkv_linear_attention(\n",
        "            self.time_decay,\n",
        "            self.time_first,\n",
        "            key,\n",
        "            value,\n",
        "            state=layer_state,\n",
        "            return_state=use_cache,\n",
        "        )\n",
        "\n",
        "        if layer_state is not None:\n",
        "            state[2][:, :, self.layer_id] = layer_state[0]\n",
        "            state[3][:, :, self.layer_id] = layer_state[1]\n",
        "            state[4][:, :, self.layer_id] = layer_state[2]\n",
        "\n",
        "        return self.output(receptance * rwkv), state\n",
        "\n",
        "\n",
        "class RwkvFeedForward(nn.Module):\n",
        "    def __init__(self, config, layer_id=0):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer_id = layer_id\n",
        "        hidden_size = config.hidden_size\n",
        "        intermediate_size = (\n",
        "            config.intermediate_size if config.intermediate_size is not None else 4 * config.hidden_size\n",
        "        )\n",
        "\n",
        "        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n",
        "        self.time_mix_key = nn.Parameter(torch.empty(1, 1, hidden_size))\n",
        "        self.time_mix_receptance = nn.Parameter(torch.empty(1, 1, hidden_size))\n",
        "\n",
        "        self.key = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
        "        self.receptance = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        self.value = nn.Linear(intermediate_size, hidden_size, bias=False)\n",
        "\n",
        "    def forward(self, hidden, state=None):\n",
        "        if hidden.size(1) == 1 and state is not None:\n",
        "            shifted = state[0][:, :, self.layer_id]\n",
        "        else:\n",
        "            shifted = self.time_shift(hidden)\n",
        "            if state is not None:\n",
        "                shifted[:, 0] = state[0][:, :, self.layer_id]\n",
        "        key = hidden * self.time_mix_key + shifted * (1 - self.time_mix_key)\n",
        "        receptance = hidden * self.time_mix_receptance + shifted * (1 - self.time_mix_receptance)\n",
        "\n",
        "        key = torch.square(torch.relu(self.key(key)))\n",
        "        value = self.value(key)\n",
        "        receptance = torch.sigmoid(self.receptance(receptance))\n",
        "\n",
        "        if state is not None:\n",
        "            state[0][:, :, self.layer_id] = hidden[:, -1]\n",
        "\n",
        "        return receptance * value, state\n",
        "\n",
        "\n",
        "class RwkvBlock(nn.Module):\n",
        "    def __init__(self, config, layer_id):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer_id = layer_id\n",
        "\n",
        "        if layer_id == 0:\n",
        "            self.pre_ln = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n",
        "\n",
        "        self.ln1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n",
        "        self.ln2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n",
        "\n",
        "        self.attention = RwkvSelfAttention(config, layer_id)\n",
        "        self.feed_forward = RwkvFeedForward(config, layer_id)\n",
        "\n",
        "    def forward(self, hidden, state=None, use_cache=False, output_attentions=False):\n",
        "        if self.layer_id == 0:\n",
        "            hidden = self.pre_ln(hidden)\n",
        "\n",
        "        attention, state = self.attention(self.ln1(hidden), state=state, use_cache=use_cache)\n",
        "        hidden = hidden + attention\n",
        "\n",
        "        feed_forward, state = self.feed_forward(self.ln2(hidden), state=state)\n",
        "        hidden = hidden + feed_forward\n",
        "\n",
        "        outputs = (hidden, state)\n",
        "        if output_attentions:\n",
        "            outputs += (attention,)\n",
        "        else:\n",
        "            outputs += (None,)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class RwkvPreTrainedModel(PreTrainedModel):\n",
        "    \"\"\"\n",
        "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
        "    models.\n",
        "    \"\"\"\n",
        "\n",
        "    config_class = RwkvConfig\n",
        "    base_model_prefix = \"rwkv\"\n",
        "    _no_split_modules = [\"RwkvBlock\"]\n",
        "    _keep_in_fp32_modules = [\"time_decay\", \"time_first\"]\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"Initialize the weights.\"\"\"\n",
        "        if isinstance(module, RwkvSelfAttention):\n",
        "            layer_id = module.layer_id\n",
        "            num_hidden_layers = module.config.num_hidden_layers\n",
        "            hidden_size = module.config.hidden_size\n",
        "            attention_hidden_size = module.attention_hidden_size\n",
        "\n",
        "            ratio_0_to_1 = layer_id / (num_hidden_layers - 1)  # 0 to 1\n",
        "            ratio_1_to_almost0 = 1.0 - (layer_id / num_hidden_layers)  # 1 to ~0\n",
        "\n",
        "            time_weight = torch.tensor(\n",
        "                [i / hidden_size for i in range(hidden_size)],\n",
        "                dtype=module.time_mix_key.dtype,\n",
        "                device=module.time_mix_key.device,\n",
        "            )\n",
        "            time_weight = time_weight[None, None, :]\n",
        "\n",
        "            decay_speed = [\n",
        "                -5 + 8 * (h / (attention_hidden_size - 1)) ** (0.7 + 1.3 * ratio_0_to_1)\n",
        "                for h in range(attention_hidden_size)\n",
        "            ]\n",
        "            decay_speed = torch.tensor(decay_speed, dtype=module.time_decay.dtype, device=module.time_decay.device)\n",
        "            zigzag = (\n",
        "                torch.tensor(\n",
        "                    [(i + 1) % 3 - 1 for i in range(attention_hidden_size)],\n",
        "                    dtype=module.time_first.dtype,\n",
        "                    device=module.time_first.device,\n",
        "                )\n",
        "                * 0.5\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                module.time_decay.data = decay_speed\n",
        "                module.time_first.data = torch.ones_like(module.time_first * math.log(0.3) + zigzag)\n",
        "\n",
        "                module.time_mix_key.data = torch.pow(time_weight, ratio_1_to_almost0)\n",
        "                module.time_mix_value.data = torch.pow(time_weight, ratio_1_to_almost0) + 0.3 * ratio_0_to_1\n",
        "                module.time_mix_receptance.data = torch.pow(time_weight, 0.5 * ratio_1_to_almost0)\n",
        "        elif isinstance(module, RwkvFeedForward):\n",
        "            layer_id = module.layer_id\n",
        "            num_hidden_layers = module.config.num_hidden_layers\n",
        "            hidden_size = module.config.hidden_size\n",
        "\n",
        "            ratio_1_to_almost0 = 1.0 - (layer_id / num_hidden_layers)  # 1 to ~0\n",
        "\n",
        "            time_weight = torch.tensor(\n",
        "                [i / hidden_size for i in range(hidden_size)],\n",
        "                dtype=module.time_mix_key.dtype,\n",
        "                device=module.time_mix_key.device,\n",
        "            )\n",
        "            time_weight = time_weight[None, None, :]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                module.time_mix_key.data = torch.pow(time_weight, ratio_1_to_almost0)\n",
        "                module.time_mix_receptance.data = torch.pow(time_weight, ratio_1_to_almost0)\n",
        "\n",
        "    def _set_gradient_checkpointing(self, module, value=False):\n",
        "        if isinstance(module, RwkvModel):\n",
        "            module.gradient_checkpointing = value\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class RwkvOutput(ModelOutput):\n",
        "    \"\"\"\n",
        "    Class for the RWKV model outputs.\n",
        "\n",
        "    Args:\n",
        "        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n",
        "            Sequence of hidden-states at the output of the last layer of the model.\n",
        "        state (list of five `torch.FloatTensor` of shape `(batch_size, hidden_size, num_hidden_layers)`):\n",
        "            The state of the model at the last time step. Can be used in a forward method with the next `input_ids` to\n",
        "            avoid providing the old `input_ids`.\n",
        "        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
        "            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
        "            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
        "\n",
        "            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n",
        "        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n",
        "            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
        "            sequence_length)`.\n",
        "\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
        "            heads.\n",
        "    \"\"\"\n",
        "\n",
        "    last_hidden_state: torch.FloatTensor = None\n",
        "    state: Optional[List[torch.FloatTensor]] = None\n",
        "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
        "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class RwkvCausalLMOutput(ModelOutput):\n",
        "    \"\"\"\n",
        "    Base class for causal language model (or autoregressive) outputs.\n",
        "\n",
        "    Args:\n",
        "        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n",
        "            Language modeling loss (for next-token prediction).\n",
        "        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n",
        "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
        "        state (list of five `torch.FloatTensor` of shape `(batch_size, hidden_size, num_hidden_layers)`):\n",
        "            The state of the model at the last time step. Can be used in a forward method with the next `input_ids` to\n",
        "            avoid providing the old `input_ids`.\n",
        "        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
        "            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n",
        "            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n",
        "\n",
        "            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n",
        "        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n",
        "            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n",
        "            sequence_length)`.\n",
        "\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
        "            heads.\n",
        "    \"\"\"\n",
        "\n",
        "    loss: Optional[torch.FloatTensor] = None\n",
        "    logits: torch.FloatTensor = None\n",
        "    state: Optional[List[torch.FloatTensor]] = None\n",
        "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
        "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
        "\n",
        "\n",
        "RWKV_START_DOCSTRING = r\"\"\"\n",
        "\n",
        "    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n",
        "    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n",
        "    etc.)\n",
        "\n",
        "    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n",
        "    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n",
        "    and behavior.\n",
        "\n",
        "    Parameters:\n",
        "        config ([`RwkvConfig`]): Model configuration class with all the parameters of the model.\n",
        "            Initializing with a config file does not load the weights associated with the model, only the\n",
        "            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
        "\"\"\"\n",
        "\n",
        "RWKV_INPUTS_DOCSTRING = r\"\"\"\n",
        "    Args:\n",
        "        input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n",
        "            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\n",
        "            `past_key_values[0][0].shape[-2]` (`sequence_length` of input past key value states). Indices of input\n",
        "            sequence tokens in the vocabulary.\n",
        "\n",
        "            If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n",
        "            `input_ids`.\n",
        "\n",
        "            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
        "            [`PreTrainedTokenizer.__call__`] for details.\n",
        "\n",
        "            [What are input IDs?](../glossary#input-ids)\n",
        "        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
        "            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n",
        "            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n",
        "            model's internal embedding lookup matrix.\n",
        "        state (tuple of five `torch.FloatTensor` of shape `(batch_size, hidden_size, num_hidden_layers)`, *optional*):\n",
        "            If passed along, the model uses the previous state in all the blocks (which will give the output for the\n",
        "            `input_ids` provided as if the model add `state_input_ids + input_ids` as context).\n",
        "        use_cache (`bool`, *optional*):\n",
        "            If set to `True`, the last state is returned and can be used to quickly generate the next logits.\n",
        "        output_attentions (`bool`, *optional*):\n",
        "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
        "            tensors for more detail.\n",
        "        output_hidden_states (`bool`, *optional*):\n",
        "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
        "            more detail.\n",
        "        return_dict (`bool`, *optional*):\n",
        "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"The bare RWKV Model transformer outputting raw hidden-states without any specific head on top.\",\n",
        "    RWKV_START_DOCSTRING,\n",
        ")\n",
        "class RwkvModel(RwkvPreTrainedModel):\n",
        "    def __init__(self, config, input_size=6, output_size=1, dropout=0.2):\n",
        "        super().__init__(config)\n",
        "\n",
        "        # self.embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "        self.fc1 = nn.Linear(input_size, config.hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.blocks = nn.ModuleList([RwkvBlock(config, layer_id=idx) for idx in range(config.num_hidden_layers)])\n",
        "        self.ln_out = nn.LayerNorm(config.hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.head = nn.Linear(config.hidden_size, output_size) # we want 1 value at the end (prediction)\n",
        "\n",
        "        self.layers_are_rescaled = False\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embeddings\n",
        "\n",
        "    def set_input_embeddings(self, new_embeddings):\n",
        "        self.embeddings = new_embeddings\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(RWKV_INPUTS_DOCSTRING)\n",
        "    @add_code_sample_docstrings(\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=RwkvOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        state: Optional[List[torch.FloatTensor]] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = True,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, RwkvOutput]:\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        use_cache = use_cache if use_cache is not None else (self.config.use_cache if not self.training else False)\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if self.training == self.layers_are_rescaled:\n",
        "            self._rescale_layers()\n",
        "\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif input_ids is None and inputs_embeds is None:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        # if inputs_embeds is None:\n",
        "        #     inputs_embeds = self.embeddings(input_ids)\n",
        "\n",
        "        if use_cache and state is None:\n",
        "            shape = (input_ids.size(0), self.config.hidden_size, self.config.num_hidden_layers)\n",
        "            state = [\n",
        "                torch.zeros(\n",
        "                    *shape, dtype=input_ids.dtype if i <= 1 else torch.float32, device=input_ids.device\n",
        "                )\n",
        "                for i in range(5)\n",
        "            ]\n",
        "            state[4] -= 1e30\n",
        "\n",
        "        hidden_states = self.fc1(input_ids)\n",
        "\n",
        "        hidden_states = self.relu(hidden_states)\n",
        "\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        for idx, block in enumerate(self.blocks):\n",
        "            hidden_states, state, attentions = block(\n",
        "                hidden_states, state=state, use_cache=use_cache, output_attentions=output_attentions\n",
        "            )\n",
        "            if (\n",
        "                self.layers_are_rescaled\n",
        "                and self.config.rescale_every > 0\n",
        "                and (idx + 1) % self.config.rescale_every == 0\n",
        "            ):\n",
        "                hidden_states = hidden_states / 2\n",
        "\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            if output_attentions:\n",
        "                all_self_attentions = all_self_attentions + (attentions,)\n",
        "\n",
        "        hidden_states = self.ln_out(hidden_states)\n",
        "\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "\n",
        "        hidden_states = self.head(hidden_states)\n",
        "\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        if not return_dict:\n",
        "            return (hidden_states, state, all_hidden_states, all_self_attentions)\n",
        "\n",
        "        return RwkvOutput(\n",
        "            last_hidden_state=hidden_states,\n",
        "            state=state,\n",
        "            hidden_states=all_hidden_states,\n",
        "            attentions=all_self_attentions,\n",
        "        )\n",
        "\n",
        "    def _rescale_layers(self):\n",
        "        # Layers should be rescaled for inference only.\n",
        "        if self.layers_are_rescaled == (not self.training):\n",
        "            return\n",
        "        if self.config.rescale_every > 0:\n",
        "            with torch.no_grad():\n",
        "                for block_id, block in enumerate(self.blocks):\n",
        "                    if self.training:\n",
        "                        block.attention.output.weight.mul_(2 ** int(block_id // self.config.rescale_every))\n",
        "                        block.feed_forward.value.weight.mul_(2 ** int(block_id // self.config.rescale_every))\n",
        "                    else:\n",
        "                        block.attention.output.weight.div_(2 ** int(block_id // self.config.rescale_every))\n",
        "                        block.feed_forward.value.weight.div_(2 ** int(block_id // self.config.rescale_every))\n",
        "\n",
        "        self.layers_are_rescaled = not self.training\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    The RWKV Model transformer with a language modeling head on top (linear layer with weights tied to the input\n",
        "    embeddings).\n",
        "    \"\"\",\n",
        "    RWKV_START_DOCSTRING,\n",
        ")\n",
        "class RwkvForCausalLM(RwkvPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.rwkv = RwkvModel(config)\n",
        "        self.head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.head\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.head = new_embeddings\n",
        "\n",
        "    def prepare_inputs_for_generation(self, input_ids, state=None, inputs_embeds=None, **kwargs):\n",
        "        # only last token for inputs_ids if the state is passed along.\n",
        "        if state is not None:\n",
        "            input_ids = input_ids[:, -1].unsqueeze(-1)\n",
        "\n",
        "        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n",
        "        if inputs_embeds is not None and state is None:\n",
        "            model_inputs = {\"inputs_embeds\": inputs_embeds}\n",
        "        else:\n",
        "            model_inputs = {\"input_ids\": input_ids}\n",
        "\n",
        "        model_inputs[\"state\"] = state\n",
        "        return model_inputs\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(RWKV_INPUTS_DOCSTRING)\n",
        "    @add_code_sample_docstrings(\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=RwkvCausalLMOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        attention_mask: Optional[torch.LongTensor] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        state: Optional[List[torch.FloatTensor]] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, RwkvCausalLMOutput]:\n",
        "        r\"\"\"\n",
        "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n",
        "            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n",
        "            are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        rwkv_outputs = self.rwkv(\n",
        "            input_ids,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            state=state,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        hidden_states = rwkv_outputs[0]\n",
        "\n",
        "        logits = self.head(hidden_states)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            # move labels to correct device to enable model parallelism\n",
        "            labels = labels.to(logits.device)\n",
        "            # Shift so that tokens < n predict n\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "            # Flatten the tokens\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + rwkv_outputs[1:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return RwkvCausalLMOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            state=rwkv_outputs.state,\n",
        "            hidden_states=rwkv_outputs.hidden_states,\n",
        "            attentions=rwkv_outputs.attentions,\n",
        "        )\n"
      ],
      "metadata": {
        "id": "Q3fDrDbFNRC0",
        "execution": {
          "iopub.status.busy": "2023-06-24T11:24:37.722038Z",
          "iopub.execute_input": "2023-06-24T11:24:37.722612Z",
          "iopub.status.idle": "2023-06-24T11:24:56.091199Z",
          "shell.execute_reply.started": "2023-06-24T11:24:37.722580Z",
          "shell.execute_reply": "2023-06-24T11:24:56.090280Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Config"
      ],
      "metadata": {
        "id": "OA-9-mVy-7W5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###======= Config Dictionary =======###\n",
        "config = {\n",
        "    \"alpha_vantage\": {\n",
        "        \"key\": \"JG7TR7PA6FKE0DCG\",\n",
        "        \"symbol\": \"MSFT\",\n",
        "        \"outputsize\": \"full\",\n",
        "        \"key_adjusted_close\": \"5. adjusted close\",\n",
        "        \"key_open\": \"1. open\",\n",
        "        \"key_high\": \"2. high\",\n",
        "        \"key_low\": \"3. low\",\n",
        "        \"key_close\": \"4. close\",\n",
        "        \"key_volume\": \"6. volume\"\n",
        "    },\n",
        "    \"data\": {\n",
        "        \"window_size\": 64,\n",
        "        \"train_split_size\": 0.8,\n",
        "        \"val_split_size\": 0.1,\n",
        "        \"test_split_size\": 0.1\n",
        "    },\n",
        "    \"plots\": {\n",
        "        \"xticks_interval\": 90, # show a date every 90 days\n",
        "        \"color_actual\": \"#001f3f\",\n",
        "        \"color_train\": \"#3D9970\",\n",
        "        \"color_val\": \"#0074D9\",\n",
        "        \"color_test\": \"#561F78\",\n",
        "        \"color_pred_train\": \"#3D9970\",\n",
        "        \"color_pred_val\": \"#0074D9\",\n",
        "        \"color_pred_test\": \"#FF4136\",\n",
        "    },\n",
        "    \"model\": {\n",
        "        \"input_size\": 1, # since we are only using 1 feature, close price\n",
        "        \"num_lstm_layers\": 2,\n",
        "        \"lstm_size\": 32,\n",
        "        \"dropout\": 0.2,\n",
        "    },\n",
        "    \"training\": {\n",
        "        \"device\": \"cpu\", # \"cuda\" or \"cpu\"\n",
        "        \"batch_size\": 128,\n",
        "        \"num_epoch\": 100,\n",
        "        \"learning_rate\": 0.01,\n",
        "        \"scheduler_step_size\": 40,\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "VSjlBANsMf3Q",
        "execution": {
          "iopub.status.busy": "2023-06-24T11:25:30.251272Z",
          "iopub.execute_input": "2023-06-24T11:25:30.251675Z",
          "iopub.status.idle": "2023-06-24T11:25:30.259659Z",
          "shell.execute_reply.started": "2023-06-24T11:25:30.251644Z",
          "shell.execute_reply": "2023-06-24T11:25:30.258736Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Pre-Processing"
      ],
      "metadata": {
        "id": "AFhrSgIi_AE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ko = yf.download(tickers=\"ko\")\n",
        "df_ko.to_csv(\"KO.csv\")\n",
        "df_ko = pd.read_csv(\"KO.csv\")\n",
        "df_ko = df_ko[5000:]\n",
        "print(df_ko)\n",
        "df_ko.plot(x=\"Date\", y=\"Adj Close\", kind=\"line\", grid=True, color=\"black\", figsize=(25, 5))  # Example for a line plot\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Adjusted Close Price\")\n",
        "plt.title(\"Adjusted Close Price of Coca-Cola stock vs. Date\")\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-24T11:44:52.644254Z",
          "iopub.execute_input": "2023-06-24T11:44:52.644684Z",
          "iopub.status.idle": "2023-06-24T11:44:53.914381Z",
          "shell.execute_reply.started": "2023-06-24T11:44:52.644653Z",
          "shell.execute_reply": "2023-06-24T11:44:53.913519Z"
        },
        "trusted": true,
        "id": "GpToaJEmTJkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df_avgd = df_ko[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']].rolling(10).mean()\n",
        "print(data_df_avgd)\n",
        "# Drop all rows with NaN values\n",
        "data_df_avgd.dropna(how='any', axis=0, inplace=True)\n",
        "data_raw = data_df_avgd.to_numpy()\n",
        "print(data_raw.shape)\n",
        "num_data_points = data_raw.shape[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-06-24T11:48:19.442544Z",
          "iopub.execute_input": "2023-06-24T11:48:19.443083Z",
          "iopub.status.idle": "2023-06-24T11:48:19.473540Z",
          "shell.execute_reply.started": "2023-06-24T11:48:19.443042Z",
          "shell.execute_reply": "2023-06-24T11:48:19.472615Z"
        },
        "trusted": true,
        "id": "hQ2Uayn_TJk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batchify(data, bsz):\n",
        "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
        "    that wouldn't cleanly fit.\n",
        "\n",
        "    Args:\n",
        "        data: Tensor, shape [N]\n",
        "        bsz: int, batch size\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape [N // bsz, bsz]\n",
        "    \"\"\"\n",
        "    seq_len = data.size(0) // bsz\n",
        "    data = data[:seq_len * bsz]\n",
        "    data = data.view(bsz, seq_len).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "QpcKMIa4UPFy",
        "execution": {
          "iopub.status.busy": "2023-06-23T19:50:25.337505Z",
          "iopub.execute_input": "2023-06-23T19:50:25.338655Z",
          "iopub.status.idle": "2023-06-23T19:50:25.345376Z",
          "shell.execute_reply.started": "2023-06-23T19:50:25.338606Z",
          "shell.execute_reply": "2023-06-23T19:50:25.343979Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "scaler = StandardScaler()\n",
        "scaler_model = scaler.fit(data_raw)\n",
        "normalized_data = scaler_model.transform(data_raw)\n",
        "mean = scaler.mean_[4]\n",
        "scale = scaler.scale_[4]\n",
        "\n",
        "mm_scaler = MinMaxScaler()\n",
        "mm_scaler_model = mm_scaler.fit(data_raw)\n",
        "normalized_data = mm_scaler_model.transform(data_raw)\n",
        "min_val = mm_scaler.data_min_[4]\n",
        "mm_scale = mm_scaler.scale_[4]\n",
        "\n",
        "def inv_transform_aclosing(x: np.ndarray):\n",
        "    return ((x*scale) + mean)\n",
        "\n",
        "def inv_transform_minmax(x: np.ndarray):\n",
        "    return ((x/mm_scale) + min_val)\n"
      ],
      "metadata": {
        "id": "_01QPfJ3u7wG",
        "execution": {
          "iopub.status.busy": "2023-06-24T11:49:04.639631Z",
          "iopub.execute_input": "2023-06-24T11:49:04.640036Z",
          "iopub.status.idle": "2023-06-24T11:49:04.653779Z",
          "shell.execute_reply.started": "2023-06-24T11:49:04.640008Z",
          "shell.execute_reply": "2023-06-24T11:49:04.652684Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data_x(x: np.ndarray, window_size):\n",
        "    # perform windowing\n",
        "    n_row = x.shape[0] - window_size + 1\n",
        "    output = np.lib.stride_tricks.as_strided(x, shape=(n_row, window_size), strides=(x.strides[0], x.strides[0]))\n",
        "    return output[:-1], output[-1]\n",
        "\n",
        "def prepare_data_y(x, window_size):\n",
        "    # use the next day as label\n",
        "    output = x[window_size:]\n",
        "    return output\n",
        "\n",
        "def all_cols_to_matrices(data: np.ndarray, window_size):\n",
        "    num_cols = data.shape[1]\n",
        "    mat_x = []\n",
        "    mat_x_unseen = []\n",
        "    for i in range(num_cols):\n",
        "        row = data[:,i].T\n",
        "        row_x, row_x_unseen = prepare_data_x(row, window_size)\n",
        "        mat_x.append(row_x)\n",
        "        mat_x_unseen.append(row_x_unseen)\n",
        "    return np.array(mat_x), np.array(mat_x_unseen)\n",
        "\n",
        "\n",
        "data_x, data_x_unseen = all_cols_to_matrices(normalized_data, window_size=config[\"data\"][\"window_size\"])\n",
        "data_y = prepare_data_y(normalized_data, window_size=config[\"data\"][\"window_size\"])\n",
        "\n",
        "data_x = np.moveaxis(data_x, 0, 2)\n",
        "\n",
        "# Split dataset\n",
        "split_index_train = int(data_y.shape[0] * config[\"data\"][\"train_split_size\"])\n",
        "split_index_val = int(data_y.shape[0] * config[\"data\"][\"val_split_size\"]) + split_index_train\n",
        "\n",
        "\n",
        "data_x_train = data_x[:split_index_train]\n",
        "data_x_val =   data_x[split_index_train:split_index_val]\n",
        "data_x_test =  data_x[split_index_val:]\n",
        "\n",
        "data_y_train = data_y[:split_index_train]\n",
        "data_y_val = data_y[split_index_train:split_index_val]\n",
        "data_y_test = data_y[split_index_val:]\n",
        "\n",
        "\n",
        "to_plot_data_y_train = np.zeros(num_data_points)\n",
        "to_plot_data_y_val = np.zeros(num_data_points)\n",
        "to_plot_data_y_test = np.zeros(num_data_points)\n",
        "\n",
        "to_plot_data_y_train[config[\"data\"][\"window_size\"]:split_index_train+config[\"data\"][\"window_size\"]] = mm_scaler.inverse_transform(data_y_train)[:, 4]\n",
        "to_plot_data_y_val[split_index_train+config[\"data\"][\"window_size\"]:split_index_val+config[\"data\"][\"window_size\"]] = mm_scaler.inverse_transform(data_y_val)[:, 4]\n",
        "to_plot_data_y_test[split_index_val+config[\"data\"][\"window_size\"]:] = mm_scaler.inverse_transform(data_y_test)[:, 4]"
      ],
      "metadata": {
        "id": "scUxZWi6Mf3R",
        "execution": {
          "iopub.status.busy": "2023-06-24T11:49:13.570915Z",
          "iopub.execute_input": "2023-06-24T11:49:13.571365Z",
          "iopub.status.idle": "2023-06-24T11:49:13.599560Z",
          "shell.execute_reply.started": "2023-06-24T11:49:13.571328Z",
          "shell.execute_reply": "2023-06-24T11:49:13.598048Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_x.shape)\n",
        "print(data_x_val.shape)\n",
        "print(normalized_data.shape)"
      ],
      "metadata": {
        "id": "no5gR3gzBuQK",
        "execution": {
          "iopub.status.busy": "2023-06-24T11:49:19.509480Z",
          "iopub.execute_input": "2023-06-24T11:49:19.510226Z",
          "iopub.status.idle": "2023-06-24T11:49:19.515977Z",
          "shell.execute_reply.started": "2023-06-24T11:49:19.510193Z",
          "shell.execute_reply": "2023-06-24T11:49:19.514843Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_plot_data_y_train = np.where(to_plot_data_y_train == 0, None, to_plot_data_y_train)\n",
        "to_plot_data_y_val = np.where(to_plot_data_y_val == 0, None, to_plot_data_y_val)\n",
        "to_plot_data_y_test = np.where(to_plot_data_y_test == 0, None, to_plot_data_y_test)\n",
        "\n",
        "## plots\n",
        "data_date = np.arange(to_plot_data_y_train.shape[0])\n",
        "fig = figure(figsize=(25, 5), dpi=80)\n",
        "fig.patch.set_facecolor((1.0, 1.0, 1.0))\n",
        "plt.plot(data_date, to_plot_data_y_train, label=\"Prices (train)\", color=config[\"plots\"][\"color_train\"])\n",
        "plt.plot(data_date, to_plot_data_y_val, label=\"Prices (validation)\", color=config[\"plots\"][\"color_val\"])\n",
        "plt.plot(data_date, to_plot_data_y_test, label=\"Prices (test)\", color=config[\"plots\"][\"color_test\"])\n",
        "xticks = [data_date[i] if ((i%config[\"plots\"][\"xticks_interval\"]==0 and (num_data_points-i) > config[\"plots\"][\"xticks_interval\"]) or i==num_data_points-1) else None for i in range(num_data_points)] # make x ticks nice\n",
        "x = np.arange(0,len(xticks))\n",
        "plt.xticks(x, xticks, rotation='vertical')\n",
        "plt.title(\"Daily close prices for \" + config[\"alpha_vantage\"][\"symbol\"] + \" - showing training and validation data\")\n",
        "plt.grid(which='major', axis='y', linestyle='--')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YJI5V5sgMf3S",
        "execution": {
          "iopub.status.busy": "2023-06-24T11:50:20.068586Z",
          "iopub.execute_input": "2023-06-24T11:50:20.069181Z",
          "iopub.status.idle": "2023-06-24T11:50:54.394214Z",
          "shell.execute_reply.started": "2023-06-24T11:50:20.069148Z",
          "shell.execute_reply": "2023-06-24T11:50:54.393260Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(source, i, bptt):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape [full_seq_len, batch_size]\n",
        "        i: int\n",
        "        bptt: int\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
        "        target has shape [seq_len * batch_size]\n",
        "    \"\"\"\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i + seq_len]\n",
        "    target = source[i+1:i+1+seq_len].ravel()\n",
        "    return data, target"
      ],
      "metadata": {
        "id": "JRDEpzkpMf3S",
        "execution": {
          "iopub.status.busy": "2023-06-24T11:51:11.618438Z",
          "iopub.execute_input": "2023-06-24T11:51:11.618976Z",
          "iopub.status.idle": "2023-06-24T11:51:11.625222Z",
          "shell.execute_reply.started": "2023-06-24T11:51:11.618924Z",
          "shell.execute_reply": "2023-06-24T11:51:11.624356Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model definitions and hyper-params"
      ],
      "metadata": {
        "id": "DcQAzJBQ_S-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "DqVG2sadTF5p",
        "execution": {
          "iopub.status.busy": "2023-06-24T11:51:16.348055Z",
          "iopub.execute_input": "2023-06-24T11:51:16.348415Z",
          "iopub.status.idle": "2023-06-24T11:51:16.353705Z",
          "shell.execute_reply.started": "2023-06-24T11:51:16.348386Z",
          "shell.execute_reply": "2023-06-24T11:51:16.352615Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x.astype(np.float32)\n",
        "        self.y = y.astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.x[idx], self.y[idx])\n",
        "\n",
        "\n",
        "dataset_train = TimeSeriesDataset(data_x_train, data_y_train)\n",
        "dataset_val = TimeSeriesDataset(data_x_val, data_y_val)\n",
        "dataset_test = TimeSeriesDataset(data_x_test, data_y_test)\n",
        "\n",
        "print(\"Train data shape\", dataset_train.x.shape, dataset_train.y.shape)\n",
        "print(\"Validation data shape\", dataset_val.x.shape, dataset_val.y.shape)\n",
        "print(\"Test data shape\", dataset_test.x.shape, dataset_test.y.shape)\n",
        "\n",
        "train_dataloader = DataLoader(dataset_train, batch_size=config[\"training\"][\"batch_size\"], shuffle=True)\n",
        "val_dataloader = DataLoader(dataset_val, batch_size=config[\"training\"][\"batch_size\"], shuffle=True)\n",
        "test_dataloader = DataLoader(dataset_test, batch_size=config[\"training\"][\"batch_size\"], shuffle=True)"
      ],
      "metadata": {
        "id": "Yea8_pKOMf3T",
        "execution": {
          "iopub.status.busy": "2023-06-24T11:51:42.024382Z",
          "iopub.execute_input": "2023-06-24T11:51:42.025329Z",
          "iopub.status.idle": "2023-06-24T11:51:42.043369Z",
          "shell.execute_reply.started": "2023-06-24T11:51:42.025290Z",
          "shell.execute_reply": "2023-06-24T11:51:42.042110Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Transformer Model\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5, norm_first=False):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout, norm_first=norm_first)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.ninp = ninp\n",
        "        self.transformer = nn.Transformer()\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src, trg, src_mask=None):\n",
        "        output = self.transformer(src, trg)\n",
        "        output = self.decoder(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "9BgUXWZJMf3T",
        "execution": {
          "iopub.status.busy": "2023-06-24T11:52:17.706014Z",
          "iopub.execute_input": "2023-06-24T11:52:17.706403Z",
          "iopub.status.idle": "2023-06-24T11:52:17.716380Z",
          "shell.execute_reply.started": "2023-06-24T11:52:17.706373Z",
          "shell.execute_reply": "2023-06-24T11:52:17.715400Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate Transformer model\n",
        "transformer_model = TransformerModel(ntoken=1, ninp=220, nhead=2, nhid=220, nlayers=8, dropout=0.2, norm_first=True)"
      ],
      "metadata": {
        "id": "UgM5iINPMf3T",
        "execution": {
          "iopub.status.busy": "2023-06-24T11:52:21.178646Z",
          "iopub.execute_input": "2023-06-24T11:52:21.179014Z",
          "iopub.status.idle": "2023-06-24T11:52:21.793683Z",
          "shell.execute_reply.started": "2023-06-24T11:52:21.178986Z",
          "shell.execute_reply": "2023-06-24T11:52:21.792610Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define RWKV model\n",
        "rwkv_config = RwkvConfig(vocab_size=0, context_length=20, hidden_size=config[\"data\"][\"window_size\"]//4, use_cache=False, num_hidden_layers=16)\n",
        "rwkv_model = RwkvModel(rwkv_config)"
      ],
      "metadata": {
        "id": "OmSC2PqRMf3U",
        "execution": {
          "iopub.status.busy": "2023-06-24T11:53:16.226027Z",
          "iopub.execute_input": "2023-06-24T11:53:16.226444Z",
          "iopub.status.idle": "2023-06-24T11:53:18.207168Z",
          "shell.execute_reply.started": "2023-06-24T11:53:16.226415Z",
          "shell.execute_reply": "2023-06-24T11:53:18.206046Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = rwkv_model\n",
        "# model = transformer_model\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "VyCOzU7LMf3U",
        "execution": {
          "iopub.status.busy": "2023-06-24T11:53:22.269809Z",
          "iopub.execute_input": "2023-06-24T11:53:22.270197Z",
          "iopub.status.idle": "2023-06-24T11:53:30.636368Z",
          "shell.execute_reply.started": "2023-06-24T11:53:22.270168Z",
          "shell.execute_reply": "2023-06-24T11:53:30.635280Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.RAdam(model.parameters(), lr=5e-2)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
      ],
      "metadata": {
        "id": "fmJlVKu-Mf3U",
        "execution": {
          "iopub.status.busy": "2023-06-24T11:53:48.744749Z",
          "iopub.execute_input": "2023-06-24T11:53:48.745122Z",
          "iopub.status.idle": "2023-06-24T11:53:48.752248Z",
          "shell.execute_reply.started": "2023-06-24T11:53:48.745092Z",
          "shell.execute_reply": "2023-06-24T11:53:48.751015Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation"
      ],
      "metadata": {
        "id": "wESz1a8n_ZfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(data_loader):\n",
        "    data_iterator = iter(data_loader)\n",
        "    nb_batches = len(data_loader)\n",
        "    model.eval()\n",
        "    acc = 0\n",
        "    losses = 0.0\n",
        "    for _, (x, y) in enumerate(data_iterator):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        out = model(x)\n",
        "        out = out.last_hidden_state[:,-1,0]\n",
        "        loss = criterion(out, y[:, 4].contiguous())\n",
        "        losses += loss\n",
        "\n",
        "    print(f\"eval loss: {losses / nb_batches}\")\n",
        "    return (losses / nb_batches)\n",
        "\n",
        "\n",
        "def train(train_loader, valid_loader):\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_iterator, valid_iterator = iter(train_loader), iter(valid_loader)\n",
        "        nb_batches_train = len(train_loader)\n",
        "        train_acc = 0\n",
        "        model.train()\n",
        "        losses = 0.0\n",
        "\n",
        "        for idx, (x, y) in enumerate(train_loader):\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            out = model(x)\n",
        "            # print(out.last_hidden_state.shape)\n",
        "            out = out.last_hidden_state[:,-1,0]\n",
        "            # print(out.shape)\n",
        "            # print(y.shape)\n",
        "            loss = criterion(out, y[:, 4].contiguous())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss.backward()\n",
        "            losses += loss.item()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"epoch {epoch}: train loss: {losses / nb_batches_train}\")\n",
        "        # print(f\"training accuracy: {train_acc / nb_batches_train}\")\n",
        "        print('evaluating on validation:')\n",
        "        score = evaluate(valid_loader)\n",
        "        scheduler.step()\n",
        "    return score"
      ],
      "metadata": {
        "id": "uxBwxri9Mf3U",
        "execution": {
          "iopub.status.busy": "2023-06-24T11:53:54.917446Z",
          "iopub.execute_input": "2023-06-24T11:53:54.917855Z",
          "iopub.status.idle": "2023-06-24T11:53:54.930495Z",
          "shell.execute_reply.started": "2023-06-24T11:53:54.917826Z",
          "shell.execute_reply": "2023-06-24T11:53:54.929405Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train(train_dataloader, val_dataloader)"
      ],
      "metadata": {
        "id": "9xA9nEEUMf3U",
        "execution": {
          "iopub.status.busy": "2023-06-24T11:54:09.562621Z",
          "iopub.execute_input": "2023-06-24T11:54:09.563609Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(dataset_train, batch_size=config[\"training\"][\"batch_size\"], shuffle=False)\n",
        "val_dataloader = DataLoader(dataset_val, batch_size=config[\"training\"][\"batch_size\"], shuffle=False)\n",
        "test_dataloader = DataLoader(dataset_test, batch_size=config[\"training\"][\"batch_size\"], shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "# predict on the training data, to see how well the model managed to learn and memorize\n",
        "\n",
        "predicted_train = np.array([])\n",
        "\n",
        "for idx, (x, y) in enumerate(train_dataloader):\n",
        "    x = x.to(device)\n",
        "    out = model(x)\n",
        "    out = out.last_hidden_state[:,-1,0]\n",
        "    out = out.cpu().detach().numpy()\n",
        "    predicted_train = np.concatenate((predicted_train, out))\n",
        "\n",
        "# predict on the validation data, to see how the model does\n",
        "\n",
        "predicted_val = np.array([])\n",
        "\n",
        "for idx, (x, y) in enumerate(val_dataloader):\n",
        "    x = x.to(device)\n",
        "    out = model(x)\n",
        "    out = out.last_hidden_state[:,-1,0]\n",
        "    out = out.cpu().detach().numpy()\n",
        "    predicted_val = np.concatenate((predicted_val, out))\n",
        "\n",
        "# predict on the test data, to see how the model does\n",
        "\n",
        "predicted_test = np.array([])\n",
        "\n",
        "for idx, (x, y) in enumerate(test_dataloader):\n",
        "    x = x.to(device)\n",
        "    out = model(x)\n",
        "    out = out.last_hidden_state[:,-1,0]\n",
        "    out = out.cpu().detach().numpy()\n",
        "    predicted_test = np.concatenate((predicted_test, out))\n",
        "\n",
        "\n",
        "# prepare data for plotting\n",
        "\n",
        "to_plot_data_y_train_pred = np.zeros(num_data_points)\n",
        "to_plot_data_y_val_pred = np.zeros(num_data_points)\n",
        "to_plot_data_y_test_pred = np.zeros(num_data_points)\n",
        "\n",
        "to_plot_data_y_train_pred[config[\"data\"][\"window_size\"]:split_index_train+config[\"data\"][\"window_size\"]] = inv_transform_minmax(predicted_train)\n",
        "to_plot_data_y_val_pred[split_index_train+config[\"data\"][\"window_size\"]:split_index_val+config[\"data\"][\"window_size\"]] = inv_transform_minmax(predicted_val)\n",
        "to_plot_data_y_test_pred[split_index_val+config[\"data\"][\"window_size\"]:] = inv_transform_minmax(predicted_test)\n",
        "\n",
        "to_plot_data_y_train_pred = np.where(to_plot_data_y_train_pred == 0, None, to_plot_data_y_train_pred)\n",
        "to_plot_data_y_val_pred = np.where(to_plot_data_y_val_pred == 0, None, to_plot_data_y_val_pred)\n",
        "to_plot_data_y_test_pred = np.where(to_plot_data_y_test_pred == 0, None, to_plot_data_y_test_pred)\n",
        "\n",
        "# plots\n",
        "data_date = np.arange(to_plot_data_y_train.shape[0])\n",
        "df_ko.plot(x=\"Date\", y=\"Adj Close\", kind=\"line\", grid=True, color=\"black\", figsize=(25, 5))  # Example for a line plot\n",
        "plt.plot(data_date, to_plot_data_y_train_pred, label=\"Predicted prices (train)\", color=config[\"plots\"][\"color_pred_train\"])\n",
        "plt.plot(data_date, to_plot_data_y_val_pred, label=\"Predicted prices (validation)\", color=config[\"plots\"][\"color_pred_val\"])\n",
        "plt.plot(data_date, to_plot_data_y_test_pred, label=\"Predicted prices (test)\", color=config[\"plots\"][\"color_pred_test\"])\n",
        "plt.title(\"Compare predicted prices to actual prices\")\n",
        "plt.grid(which='major', axis='y', linestyle='--')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5e8llay9Mf3U",
        "execution": {
          "iopub.status.busy": "2023-06-23T22:18:49.783175Z",
          "iopub.execute_input": "2023-06-23T22:18:49.783681Z",
          "iopub.status.idle": "2023-06-23T22:20:24.038473Z",
          "shell.execute_reply.started": "2023-06-23T22:18:49.783648Z",
          "shell.execute_reply": "2023-06-23T22:20:24.037389Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optuna Hyper-Parameter Search"
      ],
      "metadata": {
        "id": "weJvBAi5BGxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "aOuKR0UtHgnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_model(trial):\n",
        "  # Define RWKV model\n",
        "  n_hidden_size = trial.suggest_int(\"n_hidden_size\", 4, 32)  # Number of hiddens size will be in [4, 32]\n",
        "  n_hidden_layers = trial.suggest_int(\"n_hidden_layers\", 8, 32)\n",
        "  n_context_length = trial.suggest_int(\"n_context\", 20, 60)\n",
        "  dropout = trial.suggest_float(\"dropout\", 0.1, 0.3)\n",
        "\n",
        "  rwkv_config = RwkvConfig(vocab_size=0, context_length=n_context_length, use_cache=False, hidden_size=n_hidden_size, num_hidden_layers=n_hidden_layers)\n",
        "  rwkv_model = RwkvModel(rwkv_config, dropout=dropout)\n",
        "  return rwkv_model\n"
      ],
      "metadata": {
        "id": "2WDY45bHBK-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "  model = define_model(trial).to(device)\n",
        "\n",
        "  # Generate the optimizers.\n",
        "  lr = trial.suggest_float(\"lr\", 1e-3, 1e-1, log=True)  # log=True, will use log scale to interplolate between lr\n",
        "  optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
        "  optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
        "  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95) # To be suggested too\n",
        "\n",
        "  batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128, 256])\n",
        "  epochs = trial.suggest_int(\"epochs\", 15, 20)\n",
        "\n",
        "  # Dataloaders\n",
        "  train_dataloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "  val_dataloader = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)\n",
        "  test_dataloader = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      train_iterator = iter(train_dataloader)\n",
        "      nb_batches_train = len(train_dataloader)\n",
        "      train_acc = 0\n",
        "      model.train()\n",
        "      losses = 0.0\n",
        "\n",
        "      for idx, (x, y) in enumerate(train_dataloader):\n",
        "          x = x.to(device)\n",
        "          y = y.to(device)\n",
        "          out = model(x)\n",
        "          out = out.last_hidden_state[:,-1,0]\n",
        "          loss = criterion(out, y[:, 4].contiguous())\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          loss.backward()\n",
        "          losses += loss.item()\n",
        "\n",
        "          optimizer.step()\n",
        "\n",
        "\n",
        "      data_iterator = iter(val_dataloader)\n",
        "      nb_batches = len(val_dataloader)\n",
        "      model.eval()\n",
        "      acc = 0\n",
        "      losses = 0.0\n",
        "      for _, (x, y) in enumerate(data_iterator):\n",
        "          x = x.to(device)\n",
        "          y = y.to(device)\n",
        "          out = model(x)\n",
        "          out = out.last_hidden_state[:,-1,0]\n",
        "          loss = criterion(out, y[:, 4].contiguous())\n",
        "          losses += loss\n",
        "\n",
        "      # Step scheduler\n",
        "      scheduler.step()\n",
        "\n",
        "      # Calc score and report to optuna\n",
        "      score = losses / nb_batches\n",
        "      trial.report(score, epoch)\n",
        "\n",
        "      # then, Optuna can decide if the trial should be pruned\n",
        "      # Handle pruning based on the intermediate value.\n",
        "      if trial.should_prune():\n",
        "          raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "  return score"
      ],
      "metadata": {
        "id": "r44nKUcADGTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we can run the experiment\n",
        "sampler = optuna.samplers.TPESampler()\n",
        "study = optuna.create_study(study_name=\"predict-coca-cola\", direction=\"minimize\", sampler=sampler)\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
        "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
        "\n",
        "print(\"Study statistics: \")\n",
        "print(\"  Number of finished trials: \", len(study.trials))\n",
        "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Value: \", trial.value)\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))"
      ],
      "metadata": {
        "id": "YhuXv7cXItla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optuna.visualization.plot_param_importances(study)"
      ],
      "metadata": {
        "id": "WwdZ16CgJ6pF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optuna.visualization.plot_contour(study)"
      ],
      "metadata": {
        "id": "kBD_eHe4J9W-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}